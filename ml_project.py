# -*- coding: utf-8 -*-
"""ML_project.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16Jin3L9SE-ix2bo1aTgRLlHgLhFJiKJU
"""

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import plotly.express as px
import seaborn as sns

from sklearn.metrics import accuracy_score, confusion_matrix, classification_report, mean_squared_error, mean_absolute_error, r2_score
from sklearn.linear_model import LogisticRegression
from sklearn.svm import SVC
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import RandomForestClassifier
from sklearn.naive_bayes import GaussianNB
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.model_selection import cross_val_score, train_test_split, StratifiedKFold
from sklearn.decomposition import PCA
from sklearn.neural_network import MLPClassifier
from imblearn.over_sampling import SMOTE

df = pd.read_csv('StudentPerformanceFactors.csv')

def classify_grade(score):
    if 90 <= score <= 100:
        return 'A+'
    elif 80 <= score <= 89:
        return 'A'
    elif 75 <= score <= 79:
        return 'B+'
    elif 70 <= score <= 74:
        return 'B'
    elif 65 <= score <= 69:
        return 'C+'
    elif 60 <= score <= 64:
        return 'C'
    elif 55 <= score <= 59:
        return 'D+'


df['Performance_Grades'] = df['Exam_Score'].apply(classify_grade)

df.head()

df.describe()

print('The missing value of dataset : \n', df.isnull().sum())

df['Teacher_Quality'].fillna('Medium', inplace=True)
df['Distance_from_Home'].fillna('Moderate', inplace=True)
df['Parental_Education_Level'].fillna('College', inplace=True)


df.dropna(subset=['Performance_Grades'], inplace=True)


print('The missing value of dataset:\n', df.isnull().sum())

numerical_columns = df.select_dtypes(include=[np.number]).columns.tolist()
categorical_columns = df.select_dtypes(exclude=[np.number]).columns.tolist()


plt.figure(figsize=(15, 10))
for i, feature in enumerate(numerical_columns):
    plt.subplot((len(numerical_columns) // 3) + 1, 3, i+1)
    sns.histplot(df[feature], kde=True)
    plt.title(feature)
    plt.xlabel('')
plt.tight_layout()
plt.show()

plt.figure(figsize=(12, 10))
for i, feature in enumerate(categorical_columns):
    plt.subplot((len(categorical_columns) // 3) + 1, 3, i+1)
    sns.countplot(data=df, x=feature)
    plt.title(feature)
    plt.xlabel('')
plt.tight_layout()
plt.show()

df['Performance_Grades'].value_counts()

numerical_columns = df.select_dtypes(include=[np.number])


correlation_matrix = numerical_columns.corr()

plt.figure(figsize=(12, 10))
sns.set(style="whitegrid")

heatmap = sns.heatmap(
    correlation_matrix,
    annot=True,
    fmt=".3f",
    cmap='RdPu',
    vmin= -1
)

plt.title('Enhanced Correlation Matrix', fontsize=20)
plt.xticks(rotation=45, ha='right')
plt.tight_layout()

plt.show()

high_corr = correlation_matrix.unstack().sort_values(kind="quicksort", ascending=False)
print(high_corr[(high_corr > 0.2) & (high_corr < 1)])

fig, axes = plt.subplots(5, 5, figsize=(12, 10))
axes = axes.flatten()

for i, col in enumerate(numerical_columns):
    sns.boxplot(x=df[col], ax=axes[i])
    axes[i].set_title(f'{col} Boxplot')

for j in range(i + 1, len(axes)):
    fig.delaxes(axes[j])

plt.tight_layout()
plt.show()

label_encoder = LabelEncoder()
for column in categorical_columns:
    df[column] = label_encoder.fit_transform(df[column])


df.head()

scaler = StandardScaler()


df[numerical_columns.columns] = scaler.fit_transform(df[numerical_columns.columns])


df.head()

df.drop('Exam_Score', axis=1, inplace=True)

variances = df.var().sort_values()
print(variances)

X = df.drop(columns=['Performance_Grades'])
y = df['Performance_Grades']


X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

pca = PCA(n_components=0.99)
X_train = pca.fit_transform(X_train)
X_test = pca.transform(X_test)


PCA_var = pca.explained_variance_ratio_
print('PCA Variance Ratio:', PCA_var)

plt.plot(np.cumsum(PCA_var))
plt.xlabel('Number of Components')
plt.ylabel('Variance (%)')
plt.title('PCA Variance Importance')
plt.show()


print('Number of optimal features:', pca.n_components_)

rf = RandomForestClassifier(random_state=42)
clf = rf.fit(X, y)

fimp = pd.Series(data=clf.feature_importances_, index=X.columns).sort_values(ascending=False)


plt.figure(figsize=(17,13))
plt.title("Feature importance")
ax = sns.barplot(y=fimp.index, x=fimp.values, orient='h')

top_features = fimp[:18].index.tolist()


df_top = df[top_features + ['Performance_Grades']]
df_top.head()

X = df_top.drop(columns=['Performance_Grades'])
y = df_top['Performance_Grades']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)


smote = SMOTE(random_state=42)
X_train,y_train = smote.fit_resample(X_train,y_train)

models = []
models.append(('LR', LogisticRegression()))  # Logistic Regression
models.append(('KNN', KNeighborsClassifier()))  # KNN for Classification
models.append(('CART', DecisionTreeClassifier()))  # Decision Tree for Classification
models.append(('SVM', SVC(gamma='auto')))  # Support Vector Machine
models.append(('RF', RandomForestClassifier()))  # Random Forest
models.append(('NB', GaussianNB()))  # Naive Bayes

import warnings
from sklearn.exceptions import ConvergenceWarning
warnings.filterwarnings("ignore", category=ConvergenceWarning)

results = []
names = []
for name, model in models:
    kfold = StratifiedKFold(n_splits=10, random_state=42, shuffle=True)
    cv_results = cross_val_score(model, X_train, y_train, cv=kfold, scoring='accuracy')
    results.append(cv_results)
    names.append(name)
    msg = f"{name}: {cv_results.mean():.6f} ({cv_results.std():.6f})"
    print(msg)

fig = plt.figure()
fig.suptitle('Algorithm Comparison')
ax = fig.add_subplot(111)
plt.boxplot(results)
ax.set_xticklabels(names)
plt.ylabel('Accuracy')
plt.show()

model = RandomForestClassifier(random_state=42)
model.fit(X_train, y_train)


predictions = model.predict(X_test)


accuracy = accuracy_score(y_test, predictions)
print('Accuracy:', accuracy)


mse = mean_squared_error(y_test, predictions)
rmse = np.sqrt(mse)
mae = mean_absolute_error(y_test, predictions)
r2 = r2_score(y_test, predictions)


print('Mean Squared Error:', mse)
print('Root Mean Squared Error:', rmse)
print('Mean Absolute Error:', mae)
print('R^2:', r2)

print(classification_report(y_test, predictions))


cm = confusion_matrix(y_test, predictions)
plt.figure(figsize=(10, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted')
plt.ylabel('Actual')
plt.title('Confusion Matrix')
plt.show()

